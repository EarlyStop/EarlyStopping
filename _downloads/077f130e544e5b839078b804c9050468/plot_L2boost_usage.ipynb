{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Usage of the L2-boost class\n\nWe illustrate the available methods of the L2-boost class via a small example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport EarlyStopping as es\n\nnp.random.seed(42)\nsns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating synthetic data\nTo simulate some data we consider the signals from [Stankewitz (2022)](https://arxiv.org/abs/2210.07850v1).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_size = 1000\npara_size = 1000\n\n# Gamma-sparse signals\nbeta_3 = 1 / (1 + np.arange(para_size)) ** 3\nbeta_3 = 10 * beta_3 / np.sum(np.abs(beta_3))\n\nbeta_2 = 1 / (1 + np.arange(para_size)) ** 2\nbeta_2 = 10 * beta_2 / np.sum(np.abs(beta_2))\n\nbeta_1 = 1 / (1 + np.arange(para_size))\nbeta_1 = 10 * beta_1 / np.sum(np.abs(beta_1))\n\n# S-sparse signals\nbeta_15 = np.zeros(para_size)\nbeta_15[0:15] = 1\n# beta_15[10:15] = 0.1\nbeta_15 = 10 * beta_15 / np.sum(np.abs(beta_15))\n\nbeta_60 = np.zeros(para_size)\nbeta_60[0:20] = 1\nbeta_60[20:40] = 0.5\nbeta_60[40:60] = 0.25\nbeta_60 = 10 * beta_60 / np.sum(np.abs(beta_60))\n\nbeta_90 = np.zeros(para_size)\nbeta_90[0:30] = 1\nbeta_90[30:60] = 0.5\nbeta_90[60:90] = 0.25\nbeta_90 = 10 * beta_90 / np.sum(np.abs(beta_90))\n\nfig = plt.figure(figsize=(10, 7))\nplt.ylim(0, 0.2)\nplt.plot(beta_3[0:100])\nplt.plot(beta_2[0:100])\nplt.plot(beta_1[0:100])\nplt.show()\n\nfig = plt.figure(figsize=(10, 7))\nplt.ylim(0, 1)\nplt.plot(beta_15[0:100])\nplt.plot(beta_60[0:100])\nplt.plot(beta_90[0:100])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We simulate data from a high-dimensional linear model according to one of the signals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cov = np.identity(para_size)\nsigma = np.sqrt(1)\nX = np.random.multivariate_normal(np.zeros(para_size), cov, sample_size)\nf = X @ beta_15\neps = np.random.normal(0, sigma, sample_size)\nY = f + eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical bias-variance decomposition\nBy giving the true function f to the class, we can track the theoretical bias-variance decomposition and the balanced oracle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alg = es.L2_boost(X, Y, beta_15)\nalg.get_balanced_oracle(20)\nprint(\"The balanced oracle is given by\", alg.iteration, \"with risk =\", alg.risk[alg.iteration])\nalg.iterate(50 - alg.iteration)\nclassical_oracle = np.argmin(alg.risk)\nprint(\"The classical oracle is given by\", classical_oracle, \"with risk =\", alg.risk[classical_oracle])\n\nfig = plt.figure(figsize=(10, 7))\nplt.plot(alg.bias2)\nplt.plot(alg.stochastic_error)\nplt.plot(alg.residuals)\nplt.plot(alg.risk)\nplt.ylim((0, 1.5))\nplt.xlim((0, 50))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stopping via the discrepancy principle\nThe L2-boost class provides several data driven methods to choose a boosting iteration making the right tradeoff between bias and stochastic error.\nThe first one is a stopping condition based on the discrepancy principle, which stops when the residuals become smaller than a critical value.\nTheoretically this critical value should be chosen as the noise level of the model, for which the class also provides a methods based on the scaled Lasso.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_estimate = alg.get_noise_estimate(K = 0.5)\nstopping_time = alg.get_discrepancy_stop(critical_value = noise_estimate, max_iteration=200)\nstopping_time\nprint(\"The discrepancy based early stopping time is given by\", stopping_time, \"with risk =\",\n      alg.risk[stopping_time])\n\nnp.sqrt(np.min(alg.risk) / alg.risk[stopping_time])\nalg.risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stopping via residual ratios\nAnother method is based on stopping when the ratio of consecutive residuals goes above a certain threshhold.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stopping_time = alg.get_residual_ratio_stop(max_iteration=200, K=1.2)\nprint(\"The residual ratio based early stopping time is given by\", stopping_time, \"with risk =\",\n      alg.risk[stopping_time])\n\nstopping_time = alg.get_residual_ratio_stop(max_iteration=200, K=0.2)\nprint(\"The residual ratio based early stopping time is given by\", stopping_time, \"with risk =\",\n      alg.risk[stopping_time])\n\nstopping_time = alg.get_residual_ratio_stop(max_iteration=200, K=0.1)\nprint(\"The residual ratio based early stopping time is given by\", stopping_time, \"with risk =\",\n      alg.risk[stopping_time])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classical model selection via AIC\nThe class also has a method to compute a high dimensional Akaike criterion over the boosting path up to the current iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aic_minimizer = alg.get_aic_iteration(K=2)\nprint(\"The aic-minimizer over the whole path is given by\", aic_minimizer, \"with risk =\", alg.risk[aic_minimizer])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}